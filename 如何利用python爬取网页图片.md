# å¦‚ä½•åˆ©ç”¨pythonçˆ¬å–ç½‘é¡µå›¾ç‰‡

## :fire: 1.ç›¸å…³ä¾èµ–åº“

- [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/)   å¯ä»¥ä»HTMLæˆ–XMLæ–‡ä»¶ä¸­æå–æ•°æ®çš„Pythonåº“

- [requests](https://2.python-requests.org//en/master/)  **Requests** is an elegant and simple HTTP library for Python, built for human beings. 

## :star: 2.ä»£ç å®ç°

ä»£ç ç¼–å†™è°ƒè¯•è¿‡ç¨‹ï¼Œå¯ä»¥å‚è€ƒç®€ä¹¦ğŸ“• [å°è‘£ä¸å¤ªæ‡‚](https://www.jianshu.com/p/37202bec8f97)çš„æ•™ç¨‹ï¼Œå…¶ä»£ç ç¼–å†™æ€è·¯å¦‚ä¸‹

```flow
st=>start: å¼€å§‹
e=>end: ä¿å­˜æˆ–å¤„ç†çˆ¬å»çš„ä¿¡æ¯
op1=>operation: åˆ©ç”¨è·å–çˆ¬å–çš„ç½‘é¡µç½‘å€çš„html
op2=>operation: åˆ©ç”¨BeautifulSoupå°†htmlä¿¡æ¯è½¬æ¢æˆxmlä¿¡æ¯
op3=>operation: åˆ©ç”¨BeautifulSoupæŒ‘é€‰xmlä¸­æƒ³è¦çš„ä¿¡æ¯

st->op1->op2->op3->e
```

ä¸‹é¢è´´å‡ºä»£ç å…¨æ–‡

```python
import requests
import os
from hashlib import md5
from requests.exceptions import RequestException
from bs4 import BeautifulSoup

headers = {'If-None-Match': 'W/"5cc2cd8f-2c58"',
           "Referer": "http://www.mzitu.com/all/",
           'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 UBrowser/6.1.2107.204 SafarMozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}
#è¯·æ±‚å¤´çš„è¿™ä¸ªRefererä¸€å®šè¦åŠ ï¼Œå¦¹å­ç½‘æœ‰åçˆ¬ï¼Œåæ­£ç²˜è´´å¤åˆ¶å°±è¡Œï¼Œå¤šåŠ å‡ ä¸ªä¿¡æ¯æ— æ‰€è°“
def get_page(url):
    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            #print(response.text)
            return response.text
        return None
    except RequestException:
        print('è·å–ç´¢å¼•é¡µå¤±è´¥')
        return None

def parse_page(html):
    soup = BeautifulSoup(html, 'lxml')
    items = soup.select('#pins li')
    for link in items:
        href = link.a['href']
        #print(href)
        get_detail_page(href)
    # print(items)

def get_detail_page(href):
    for i in range(1,100):
        detail_url = href + '/' + str(i)
        if requests.get(detail_url, headers=headers).status_code == 200:
            #print(detail_url)
            parse_detail_page(detail_url)
        else:
            print('å·²è‡³æœ«å°¾é¡µ')
            return None
 

def parse_detail_page(detail_url):
    try:
        response = requests.get(detail_url, headers=headers)
        if response.status_code == 200:
            print('è·å–è¯¦æƒ…é¡µæˆåŠŸ')
            detail_html = response.text
            # print(detail_html)
            get_image(detail_html)
        return None
    except RequestException:
        print('è·å–è¯¦æƒ…é¡µå¤±è´¥')
        return None

def get_image(detail_html):
    soup = BeautifulSoup(detail_html, 'lxml')
    items= soup.select('.main-image')
    # print(items)
    for item in items:
        #return item.img['src']
    	image = item.img['src']
        save_image(image)

def save_image(image):
   response = requests.get(image,headers=headers)
   if response.status_code == 200:
       data = response.content
       file_path = '{0}/{1}.{2}'.format(os.getcwd(), md5(data).hexdigest(), 'jpg')
       print(file_path)
       if not os.path.exists(file_path):
           with open(file_path, 'wb') as f:
               f.write(data)
               f.close()
               print('ä¿å­˜æˆåŠŸ')
   else:
       print('ä¿å­˜å¤±è´¥')

def main():
    url = 'https://www.mzitu.com/tag/youhuo/'
    html = get_page(url)
    parse_page(html)


if __name__ == '__main__':
    main()
```

[^1] https://www.jianshu.com/p/37202bec8f97